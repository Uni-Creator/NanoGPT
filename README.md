# NANOGPT
![GitHub Repo stars](https://img.shields.io/github/stars/Uni-Creator/NanoGPT?style=social)  ![GitHub forks](https://img.shields.io/github/forks/Uni-Creator/NanoGPT?style=social)

## ğŸ“Œ Overview
The **NANOGPT** project is a lightweight implementation of GPT-style language models. It processes text data and can generate coherent text sequences based on a trained model. The project is designed to be simple and efficient while maintaining flexibility for experimentation.

## ğŸš€ Features
- **Lightweight GPT Model**: Efficient architecture for text generation.
- **Pretrained Model Support**: Load existing `model.pth` for inference.
- **Customizable Training**: Train on different text datasets.
- **Evaluation & Testing**: Evaluate performance using test scripts.
- **Minimal Dependencies**: Simple setup without heavy frameworks.

## ğŸ—ï¸ Tech Stack
- **Python**
- **PyTorch** (for model training)
- **NumPy** (for data processing)
- **Torchvision** (for potential dataset handling)
- **Matplotlib** (for visualization)

## ğŸ“‚ Project Structure
```
NANOGPT/
â”‚â”€â”€ __pycache__/              # Cached Python files
â”‚â”€â”€ main.py                   # Loads trained model and generates text
â”‚â”€â”€ model.pth                 # Pretrained model checkpoint
â”‚â”€â”€ Nature_of_Code.pdf        # Reference material for training data
â”‚â”€â”€ shakes.txt                # Shakespeare dataset used for training
â”‚â”€â”€ test.py                   # Testing script for evaluation
â”‚â”€â”€ trainer.py                # Model training script
â”‚â”€â”€ README.md                 # Project documentation
```

## ğŸ“¦ Installation & Setup
1. **Clone the repository**
   ```sh
   git clone https://github.com/Uni-Creator/NanoGPT.git
   cd NANOGPT
   ```
2. **Install dependencies**
   ```sh
   pip install torch numpy matplotlib
   ```
3. **Train the model (if needed)**
   ```sh
   python trainer.py
   ```
4. **Run the model for text generation**
   ```sh
   python main.py
   ```

## ğŸ“Š How It Works
1. The model loads a pretrained `model.pth` or trains from scratch.
2. It processes an input text prompt.
3. The model generates a sequence of text based on learned patterns.
4. The output text is displayed and can be saved.

## ğŸ› ï¸ Future Improvements
- Implement Transformer-based architecture for better efficiency.
- Expand dataset for broader language capabilities.
- Create an interactive web-based demo.

## ğŸ¤ Contributing
Contributions are welcome! Feel free to open an **issue** or submit a **pull request**.

## ğŸ“„ License
This project is licensed under the **MIT License**.
